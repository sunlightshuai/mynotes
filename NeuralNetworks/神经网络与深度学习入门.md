# 认识神经网络

## 感知器

⼀个感知器接受几个⼆进制输⼊，x1,x2,...，并产⽣⼀个⼆进制输出。引入**权重**，w1,w2,...，表⽰相应输⼊对于输出重要性的实数

神经 元的输出，0或者1，则由分配权重后的总和∑j wj*xj⼩于或者⼤于⼀些**阈值**决定。和权重⼀样， 阈值是⼀个实数，⼀个神经元的参数。 

**简易的数理模型，解决输入，产出输出。**可聚集形成网络。

## S型神经元 

如果对权重（或者偏置）的微小的改动真的能够仅仅引起输出的微⼩变化，那我们可以利⽤这⼀事实来修改权重和偏置，让我们的网络能够表现得像我们想要的那样。

**通过S函数纠正感知器的偏差。**其局部整体可以被认为是感知器，或者可以具备感知器的功能。

## 神经网络的架构

输入层称为输入神经元。输出层包含输出神经元。中间层被称为隐藏层。

由S型神经元而不是感知器构成，这种多层网络有时被称为**多层感知器**或者**MLP**。

以上一层的输出作为下⼀层的输⼊，这种网络被称为**前馈神经网络**。这意味着网络中是没有回路的——信息总是向前传播，从不反向回馈。如果确实有回路，我们最终会有这样的情况：σ函数的输入依赖于输出。这将难于理解，所以不允许这样的环路。

也有⼀些人工神经网络的模型，其中反馈环路是可行的。这些模型被称为**递归神经网络**。这种设计思想是，休眠前会在⼀段有限的时间内保持激活状态的神经元。这种激活状态可以刺激其它神经元，使其随后被激活并同样保持⼀段有限的时间。这样会导致更多的神经元被激活，随着时间的推移，我们得到⼀个级联的神经元激活系统。因为⼀个神经元的输出只在⼀段时间后而不是即刻影响它的输⼊，在这个模型中回路并不会引起问题。 

递归神经网络比前馈网络影响力小得多，部分原因是递归网络的学习算法（至少目前为止） 不够强⼤。但是递归网络仍然很有吸引力。它们原理上比前馈网络更接近我们⼤脑的实际⼯作。 并且递归网络能解决⼀些重要的问题，这些问题如果仅仅用前馈网络来解决，则更加困难。

## 使用梯度下降算法进行学习

用符号x来表示⼀个训练输⼊。为了方便，把每个训练输⼊x看作⼀个28×28 = 784 维的向量。每个向量中的项目代表图像中单个像素的灰度值。我们用y = y(x)表示对应的期望输出，这里y是⼀个10维的向量。例如，如果有⼀个特定的画成6的训练图像，x，那么 y(x) = (0,0,0,0,0,0,1,0,0,0)T 则是网络的期望输出。注意这里T 是转置操作，把⼀个行向量转换成⼀个列向量。 

 我们希望有⼀个算法，能让我们找到权重和偏置，以⾄于网络的输出y(x)能够拟合所有的训练输入x。为了量化我们如何实现这个目标，我们定义⼀个代价函数

![1583035874(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583035874(1).png)

w表示所有的网络中权重的集合，b是所有的偏置，n是训练输⼊数据的个数，a是表示当输入为x时输出的向量，求和则是在总的训练输入x上进行的。当然，输出a取决于x, w 和b，但是为了保持符号的简洁性，我没有明确地指出这种依赖关系。符号∥v∥是指向量v的模。 我们把C称为⼆次代价函数；有时也被称为均方误差或者MSE。

我们训练神经网络的目的是找到能最小化⼆次代价函数C(w,b)的权重和偏置。 

使用梯度下降的技术来解决这样的最小化问题。

【一种解决这个问题的方式是使用微积分来解析最小值。我们可以计算导数去寻找C的极值点。 运气好的话，C是一个只有一个或少数几个变量的函数。但是变量过多的话那就是噩梦。而且神经网络中我们经常需要大量的变量——最大的神经网络有依赖数亿权重和偏置的代价函数，极其复杂。用微积分来计算最小值已经不可能了。】 

设计一个最小化C的算法，为了更精确地描述这个问题，让我们思考下，当我们在v1和v2方向分别将球体移动一个很小的量，即∆v1和∆v2时，微积分告诉我们C将会有如下变化： 

![1583072848(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583072848(1).png)

我们要寻找一种选择∆v1和∆v2的⽅法使得∆C为负；即，我们选择它们是为了让球体滚落。为了弄明白如何选择，需要定义∆v为v变化的向量，∆v ≡ (∆v1,∆v2)T，T 是转置符号。 

我们也定义C的梯度为偏导数的向量，![583072948(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583072948(1).png)，用∆C来表示梯度向量，即：

![1583073050(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583073050(1).png)



# 反向传播算法如何工作

神经网络通过梯度下降算法来学习自身的权重和偏置。但是，并没有讨论如何计算代价函数的梯度。计算这些梯度的快速算法，也就是反向传播（backpropagation）。 

## 神经网络中使用矩阵快速计算输出的方法

我们给出网络中权重的清晰定义。我们使用![1583074598(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074598(1).png)表示从![1583074655(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074655(1).png)层的![1583074699(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074699(1).png)个神经元到![1583074743(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074743(1).png)层的![1583074779(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074779(1).png)个神经元的链接上的权重。例如，下图给出了网络中第二层的第四个神经元到第三层的第二个神经元的链接上的权重：

![1583074895(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074895(1).png)



对网络的偏置和激活值也会使用类似的表示。显式地，我们使用![1583076125(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583076125(1).png)表示在![1583074743(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074743(1).png)层的第![1583074779(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074779(1).png)个神经元的偏置，使用![1583076283(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583076283(1).png)表示在![1583074743(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074743(1).png)层的第![1583074779(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583074779(1).png)个神经元的激活值。如下图：

![1583076362(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583076362(1).png)

通过方程关联如下

![1583076450(1)](D:\git_source\mynotes\NeuralNetworks\photo1\1583076450(1).png)



![](D:\git_source\mynotes\NeuralNetworks\photo1\1583417511(1).png)



# 改进神经网络的学习方法

## 交叉熵代价函数

https://zhuanlan.zhihu.com/p/70804197



# 深度学习

## 介绍卷积网络 

网络中的神经元与相邻层上的每个神经元均连接，即全连接的邻接关系的网络。

对输⼊图像中的每个像素点，将其光强度作为对应输入层神经元的值。对于 28×28像素的图像，意味着我们的网络有784（= 28×28）个输入神经元。我们训练网络的权重和偏置，使得网络输出能够——如我们希望地——正确地辨认输⼊图像。

网络架构不考虑图像的空间结构，适用于图像分类。

卷积神经网络采用了三种基本概念：**局部感受野（localreceptiveﬁelds）**，**共享权重（shared weights）**，和**混合（pooling）**。

**局部感受野**：在之前看到的全连接层的网络中，输入被描绘成纵向排列的神经元。但在⼀个卷积网络中，把输入看作是⼀个28×28的方形排列的神经元更有帮助，其值对应于我们用作输入的28×28的像素光强度。

输入图像的区域（从28×28方形排列神经元中选5×5）被称为隐藏神经元的**局部感受野**。它是输入像素上的⼀个小窗⼝。每个连接学习⼀个权重。而隐藏神经元同时也学习⼀个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。

整个输入图像上交叉移动局部感受野。对于每个局部感受野，在第⼀个隐藏层中有⼀个不同的隐藏神经元。如此重复，构建起第⼀个隐藏层。如果我们有⼀个28×28的输入图像，5×5的局部感受野，那么隐藏层中就会有24×24个神经元。这是因为在抵达右边（或者底部）的输入图像之前，我们只能把局部感受野横向移动23个神经元（或者往下23个神经元）。  

局部感受野每次移动一个像素。实际上，有时候会使⽤不同的**跨距**。例如，我可以 往右（或下）移动2个像素的局部感受野，这种情况下我们使⽤了2个跨距。

**共享权重和偏置**：每个隐藏神经元具有⼀个偏置和连接到它的局部感受野的 5×5权重。没有提及的是我们打算对24×24隐藏神经元中的每⼀个使⽤相同的权重和偏置。

对第j,k个隐藏神经元，输出为：

![1581819720](D:\git_source\mynotes\NeuralNetworks\photo1\1581819720.png)

这⾥σ是神经元的激活函数——可以是我们在前面使用过的S型函数。b是偏置的共享 值。wl,m是⼀个共享权重的5×5数组。最后，我们使⽤ax,y来表⽰位置为x,y的输⼊激活值。

这意味着第⼀个隐藏层的所有神经元检测完全相同的特征，只是在输入图像的不同位置。 要明白为什么是这个道理，把权重和偏置设想成隐藏神经元可以挑选的东西，例如，在⼀个特定的局部感受野的垂直边缘，这种能力在图像的其它位置也很可能是有用的。因此，在图像中应用相同的特征检测器是有用的。⽤稍微更抽象的术语，卷积网络能很好地适应图像的平移不变性：例如稍稍移动⼀幅猫的图像，它仍然是⼀幅猫的图像。因为这个原因，我们有时候把从输入层到隐藏层的映射称为⼀个特征映射。我们把定义特征映射的权重称为**共享权重**。我们把以这种方式定义特征映射的偏置称为**共享偏置**。共享权重和偏置经常被称为⼀个**卷积核**或者**滤波器**。

![1581837881](D:\git_source\mynotes\NeuralNetworks\photo1\1581837881.png)

如上图所示，有3个特征映射。每个特征映射定义为⼀个5×5共享权重和单个共享偏置的集合。其结果是网络能够检测3种不同的特征，每个特征都在整个图像中可检测。 

为了让上面的图示简单些，仅仅展示了3个特征映射。然而，在实践中卷积网络可能使用很多的特征映射。

共享权重和偏置的⼀个很大的优点是，它大大减少了参与的卷积网络的参数。对于每个特征映射我们需要25 = 5×5个共享权重，加上⼀个共享偏置。所以每个特征映射需要26个参数。 如果我们有20个特征映射，那么总共有20×26 = 520个参数来定义卷积层。作为对比，假设我们有⼀个全连接的第⼀层，具有784 = 28×28个输⼊神经元，和⼀个相对适中的30个隐藏神经元。总共有784×30个权重，加上额外的30个 偏置，共有23,550个参数。换句话说，这个全连接的层有多达40倍于卷基层的参数。

**混合层**：除了刚刚描述的卷积层，卷积神经网络也包含**混合层**（poolinglayers）。混合层通常紧接着在卷积层之后使⽤。它要做的是简化从卷积层输出的信息。

详细地说，⼀个混合层取得从卷积层输出的每⼀个特征映射并且从它们准备⼀个凝缩的特征映射。例如，混合层的每个单元可能概括了前⼀层的⼀个（比如）2×2的区域。作为⼀个具体的例子，⼀个常见的混合的程序被称为**最大值混合**（max-pooling）。在最大值混合中，⼀个混合单元简单地输出其2×2输⼊区域的最大激活值，正如下图说明的： 

![1581838815](D:\git_source\mynotes\NeuralNetworks\photo1\1581838815.png)

注意既然从卷积层有24×24个神经元输出，混合后我们得到12×12个神经元。 

正如上面提到的，卷积层通常包含超过一个特征映射。我们将最大值混合分别应用于每一个特征映射。所以如果有三个特征映射，组合在一起的卷积层和最大值混合层看起来像这样

![1581839043](D:\git_source\mynotes\NeuralNetworks\photo1\1581839043.png)

我们可以把最大值混合看作⼀种网络询问是否有⼀个给定的特征在⼀个图像区域中的哪个地方被发现的方式。然后它扔掉确切的位置信息。直观上，一旦一个特征被发现，它的确切位置并不如它相对于其它特征的大概位置重要。⼀个很大的好处是，这样可以有很多被更少地混合的特征，所以这有助于减少在以后的层所需的参数的数目。

最大值混合并不是用于混合的仅有的技术。另⼀个常用的方法是L2混合（L2pooling）。这里取2×2区域中激活值的平方和的平方根，而不是最大激活值。虽然细节不同，但其直观上和最大值混合是相似的：L2混合是⼀种凝缩从卷积层输出的信息的方式。在实践中，两种技术都被广泛应用。而且有时候人们使用其它混合操作的类型。

**综合在一起**：我们现在可以把这些思想都放在⼀起来构建⼀个完整的卷积神经网络。

![1581839754](D:\git_source\mynotes\NeuralNetworks\photo1\1581839754.png)

网络中最后连接的层是一个全连接层。更确切地说，这一层将最大值混合层的每一个神经元连接到每一个输出神经元。

## [分组卷积](https://www.e-learn.cn/content/qita/3369010)

分组卷积最早出现在AlexNet中，当时硬件资源有限，训练时卷积操作不能全部放在同一个GPU中运算，因此作者在2个GPU上运行，把feature maps分给这两个GPU分别进行处理，最后把这两个GPU的结果进行concatenate，作为一层的output。

直到2016年[Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups](https://arxiv.org/abs/1605.06489)的论文首次发表之前，绝大多数深度学习研究人员都将一组滤波器解释为一种工程手段。确实，这是发明卷积层的主要原因，但是如果减少卷积层的参数，准确性肯定会下降吗?

AlexNet的作者在2012年也指出了一个有趣的副作用：conv1过滤器很容易被解释，过滤器组似乎总是把conv1分成两个独立的、截然不同的任务：黑白过滤器和彩色过滤器。

AlexNet论文中没有明确指出的是分组卷积更重要的副作用，即它们能更好地学习表示。这似乎是一个很不寻常的现象，然而可以用一个简单的实验来证明的：在使用分组卷积和不使用分组卷积来训练AlexNet，并观察准确率/计算效率的差异。不用分组卷积的AlexNet不仅效率更低(在参数和计算方面)，而且准确性也稍差。

首先回顾一下普通的卷积，这里我们显式地显示通道数，输入为H×W×c1，共有c2个h×w×c1的滤波器，经过卷积后，输出为H×W×c2。

![1582272990](D:\git_source\mynotes\NeuralNetworks\photo1\1582272990.png)

接下来是分组卷积，这里group=2，即图中的g=2，可以看出，输入的c1个通道，被分为2组，每组为H×W×c1/2，卷积滤波器变成了2组，每组c2/2个大小为h×w×c1/2的滤波器，所以共有c2个h×w×c1/2的滤波器，每组中的每个滤波器只与前一层的一半特征图进行卷积。因此滤波器的参数数量(黄色)正好是对应正常卷积层的一半。

![1582273081](D:\git_source\mynotes\NeuralNetworks\photo1\1582273081.png)

比如input大小为H×W×32，通道数c1=32，想要得到output的通道数c2=64，当group是1时，要经过64个3×3×32滤波器,那么该卷积层的参数个数是64×3×3×32，即64个3×3×32大小的滤波器，pytorch中写法如下:

```python
from torch import nn
conv = nn.Conv2d(32, 64, 3, 1)
print(conv.weight.data.size())
```

结果

```python
torch.Size([64, 32, 3, 3])
```

如果把group设置为2，要么每个滤波器的channel=c1/g=32/2=16，需要64个3×3×16大小的滤波器，滤波器分成两组，每组32个3×3×16，input也被分成两组，每组的shape为H×W×16，每组做卷积运算，得到两组H×W×32，做拼接，得到H×W×64的output。参数个数是64×3×3×16，减少了一半。

```python
conv1 = nn.Conv2d(32, 64, 3, 1, groups=2)
print(conv1.weight.data.size())
```

结果

```python
torch.Size([64, 16, 3, 3])
```

如果把group设置为4，要么每个滤波器的channel=c1/g=32/4=8，需要64个3×3×8大小的滤波器，滤波器分成四组，每组16个3×3×8，input也被分成四组，每组的shape为H×W×8，每组做卷积运算，得到四组H×W×16，做拼接，得到H×W×64的output。参数个数是64×3×3×8，比分两组再减少了一半。

```python
conv2 = nn.Conv2d(32, 64, 3, 1, groups=4)
print(conv2.weight.data.size())
```

结果

```python
torch.Size([64, 8, 3, 3])
```

groups的值必须能被in_channels和out_channels整除，否则会报错。
